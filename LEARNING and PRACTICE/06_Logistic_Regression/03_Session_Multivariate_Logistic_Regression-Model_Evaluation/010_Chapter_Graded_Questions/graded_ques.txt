QUESTION 1:


Calculating Sensitivity
Suppose you got the following confusion matrix for a model by using a cutoff of 0.5.

Actual/Predicted	Not Churn	Churn
Not Churn			1200	400
Churn				350	1050
 

Calculate the sensitivity for the model above. Now suppose for the same model, you changed the cutoff from 0.5 to 0.4 such that your number of true positives increased from 1050 to 1190. What will the be the change in sensitivity?

 

Note: Report the answer in terms of new_value - old_value, i.e. if the sensitivity was, say, 0.6 earlier and then changed to 0.8, report it as (0.8 - 0.6), i.e. 0.2.


ANS:

0.1

Feedback :
Correct!

Recall that the formula for sensitivity is given by -

Sensitivity = Number of actial Yesses correctly Predicted/Total number of actual yeses
 
	    = TP/(TP +FN)

Here, TP = 1050 and FN = 350

Hence, initially, the sensitivity was -

=1050/1050+350
=75%

Now, when you changed the threshold, the number of true positives changes from 1050 to 1190. Now, since the number of actual positives will be the same always, i.e. (1050 + 350 = 1400) from the original confusion matrix, you can now calculate the new sensitivity as - 

Sensitivity = 1190/1400
=0.85

Hence, the change in sensitivity = 0.85 - 0.75 = 0.1

________________________________________________________________________________________________________

QUESTION 2:

Calculating Precision
Consider the confusion matrix you had in the last question.

Actual/Predicted	Not Churn	Churn
Not Churn	1200	400
Churn	350	1050
 

Calculate the values of precision and recall for the model and determine which of the two is higher.

ANS:

Precision =TP/TP+FP

Recall= TP/TP+FN

Here, TP = 1050; FP = 400; FN= 350

Hence, you get -

Precision =TP/TP+FP = 72.41%

Recall= TP/TP+FN = 75%

As you can see, of the two, recall is higher.


________________________________________________________________________________________________________


QUESTION 3:

True Positive Rate
Fill in the blanks.

The True Positive Rate (TPR) metric is exactly the same as ______.


ANS:

Sensitivity

Feedback :
Correct! Recall the formula for TPR is given as:

TPR =TP/TP+FN

And this is exactly the same as sensitivity as you might remember.


________________________________________________________________________________________________________


QUESTION 4:

Threshold
Suppose someone built a logistic regression model to predict whether a person has a heart disease or not. All you have from their model is the following table which contains data of 10 patients.

Patient ID	Heart Disease	Predicted Probability for Heart Disease	Predicted Label
1001	0	0.34	0
1002	1	0.58	1
1003	1	0.79	1
1004	0	0.68	1
1005	0	0.21	0
1006	0	0.04	0
1007	1	0.48	0
1008	1	0.64	1
1009	0	0.61	1
1010	1	0.86	1
 

Now, you wanted to find out the cutoff based on which the classes were predicted, but you can't. But can you identify which of the following cutoffs would be a valid cutoff for the model above based on the 10 data points given in the table? (More than one option may be correct.)

OPTIONS:

A.
0.45

B.
0.50

C.
0.55

D.
0.60


ANS:

B.
0.50

Feedback :
Yes! 

See the table carefully. For patient 1007, the predicted probability is 0.48 and the predicted class is 0. This means that the cutoff has to be greater than 0.48. Also, for patient 1002, the predicted probability is 0.58 and the predicted class is 1. This means that the cutoff has to be lesser than 0.58.

Therefore, the cutoff can lie between 0.48-0.58 and hence, 0.50 are 0.55 can be valid cutoffs for the model above.

Correct

C.
0.55

Feedback :
Yes! 

See the table carefully. For patient 1007, the predicted probability is 0.48 and the predicted class is 0. This means that the cutoff has to be greater than 0.48. Also, for patient 1002, the predicted probability is 0.58 and the predicted class is 1. This means that the cutoff has to be lesser than 0.58.

Therefore, the cutoff can lie between 0.48-0.58 and hence, 0.50 are 0.55 can be valid cutoffs for the model above.

Correct


________________________________________________________________________________________________________

QUESTION 5:

Evaluation Metrics
Consider the same model given in the last question.

Patient ID	Heart Disease	Predicted Probability for Heart Disease	Predicted Label
1001	0	0.34	0
1002	1	0.58	1
1003	1	0.79	1
1004	0	0.68	1
1005	0	0.21	0
1006	0	0.04	0
1007	1	0.48	0
1008	1	0.64	1
1009	0	0.61	1
1010	1	0.86	1
 

Calculate the values of Accuracy, Sensitivity, Specificity, and Precision. Which of these four metrics is the highest for the model?


ANS:

Sensitivity

Feedback :
From the table given above, you can easily find out that - 

TN = 3

FP = 2

FN = 1

TP = 4

Hence, your confusion matrix will look like:

Actual/Predicted	No Heart Disease	Heart Disease
No Heart Disease		3		2
Heart Disease			1		4

 
Hence, you get - 

Accuracy = 70%

Sensitivity = 80%

Specificity = 60%

Precision = 67%


